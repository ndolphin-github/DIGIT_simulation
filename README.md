# DIGIT Sensor Simulation and Deep Learning Framework

![Python](https://img.shields.io/badge/python-3.8+-blue.svg)
![PyTorch](https://img.shields.io/badge/PyTorch-2.4.1-red.svg)
![License](https://img.shields.io/badge/license-MIT-green.svg)

A comprehensive simulation and deep learning framework for DIGIT tactile sensors, featuring bidirectional neural networks for physical-visual mapping and advanced perception capabilities.

## üìä **Dataset**

The complete dataset for this project is available on Hugging Face:

üîó **[DIGIT Simulation Dataset](https://huggingface.co/datasets/Ndolphin/DIGIT_simulation/tree/main/datasets)**

### Dataset Contents:
- **NodalDataOutput.zip**: CSV files containing nodal displacement data [x, y, dz] from FEM simulations
- **DeltaImages_100x75.zip**: RGB delta images (100√ó75 resolution) showing tactile sensor changes
- **ContactMasks_100x75.zip**: Binary contact masks indicating interaction regions

### Dataset Statistics:
- **Sensors**: 3 different DIGIT sensors (D21119, D21242, D21273)
- **Objects**: 7+ different objects (hammar, mug, Rabit_L, Rabit_R, scissor, soupcan, stamp)
- **Time Steps**: ~100 interaction sequences per object
- **Total Samples**: ~150+ paired CSV-image samples for training
- **Test Data**: 8 unseen object configurations (id1-id8) with 100 steps each

### Usage:
```python
# Download dataset using Hugging Face datasets library
from huggingface_hub import snapshot_download

# Download entire dataset
snapshot_download(repo_id="Ndolphin/DIGIT_simulation", repo_type="dataset", local_dir="./DIGIT_data")

# Or download specific files
from huggingface_hub import hf_hub_download

# Download nodal data
nodal_data = hf_hub_download(repo_id="Ndolphin/DIGIT_simulation", filename="datasets/NodalDataOutput.zip", repo_type="dataset")
```

## üéØ Overview

This repository contains a complete pipeline for DIGIT tactile sensor simulation and analysis, including:

- **FEM Simulation**: SOFA-based finite element modeling of tactile interactions
- **Perception Network**: Visual-to-physical displacement mapping using U-Net
- **Rendering Network**: Physical-to-visual delta image synthesis using UNetSmall
- **Sensor Experiments**: Real DIGIT sensor data collection and processing

## üèóÔ∏è Architecture

### 1. **Perception Network** (Visual ‚Üí Physical)
- **Input**: RGB delta images (320√ó240√ó3)
- **Output**: Physical displacement fields [x, y, dz] (320√ó240√ó3)
- **Architecture**: Clean U-Net with skip connections
- **Purpose**: Extract physical sensor data from visual representations

### 2. **Rendering Network** (Physical ‚Üí Visual)
- **Input**: Multi-channel sensor data (19√ó75√ó100)
  - Displacement field (1 channel)
  - Coordinate grids (2 channels)
  - Fourier positional encoding (16 channels)
- **Output**: RGB delta images (3√ó75√ó100)
- **Architecture**: UNetSmall with masked loss
- **Purpose**: Generate realistic tactile visualizations from sensor data

### 3. **FEM Simulation**
- SOFA-based finite element modeling
- Contact simulation with various indenters
- Nodal displacement data generation

## üìÅ Repository Structure

```
DIGIT_simulation/
‚îú‚îÄ‚îÄ FEM_simulation_SOFAscene/          # SOFA-based FEM simulation
‚îÇ   ‚îú‚îÄ‚îÄ ContactTest_in_loop.py         # Batch contact simulation
‚îÇ   ‚îú‚îÄ‚îÄ ContactTest_single.py          # Single contact test
‚îÇ   ‚îú‚îÄ‚îÄ meshes/                        # 3D mesh files for simulation
‚îÇ   ‚îî‚îÄ‚îÄ NodalDataOutput/               # Generated CSV nodal data
‚îú‚îÄ‚îÄ Perception_Network/                # Visual-to-physical mapping
‚îÇ   ‚îú‚îÄ‚îÄ Train.py                       # Training script for perception
‚îÇ   ‚îú‚îÄ‚îÄ data_preprocessing.py          # Data preparation utilities
‚îÇ   ‚îú‚îÄ‚îÄ Testset_inference.py          # Inference on test data
‚îÇ   ‚îî‚îÄ‚îÄ perception_unet_model.pth      # Trained model weights
‚îú‚îÄ‚îÄ Rendering_Network/                 # Physical-to-visual synthesis
‚îÇ   ‚îú‚îÄ‚îÄ Train.py                       # Training script for rendering
‚îÇ   ‚îú‚îÄ‚îÄ data_loader.py                 # Data loading utilities
‚îÇ   ‚îú‚îÄ‚îÄ inference.py                   # Inference script
‚îÇ   ‚îî‚îÄ‚îÄ best_model.pt                  # Trained model weights
‚îú‚îÄ‚îÄ DIGIT_sensor_experiment/           # Real sensor experiments
‚îÇ   ‚îú‚îÄ‚îÄ DIGIT_CameraView.py           # Camera interface
‚îÇ   ‚îú‚îÄ‚îÄ ImageExtraction.py            # Image processing
‚îÇ   ‚îî‚îÄ‚îÄ Recorded_Videos/              # Experimental data
‚îî‚îÄ‚îÄ requirements.txt                   # Python dependencies
```

## üöÄ Quick Start

### Installation

1. **Clone the repository:**
```bash
git clone https://github.com/ndolphin-github/DIGIT_simulation.git
cd DIGIT_simulation
```

2. **Create a virtual environment:**
```bash
python -m venv digit_env
source digit_env/bin/activate  # On Windows: digit_env\Scripts\activate
```

3. **Install dependencies:**
```bash
pip install -r requirements.txt
```

### Running the Networks

#### Perception Network (Visual ‚Üí Physical)
```bash
cd Perception_Network
python Train.py --epochs 50 --batch_size 4 --learning_rate 1e-3
python Testset_inference.py --model_path perception_unet_model.pth
```

#### Rendering Network (Physical ‚Üí Visual)
```bash
cd Rendering_Network
python Train.py --epochs 50 --batch_size 8 --lr 2e-4
python inference.py -m best_model.pt -u NodalDataOutput -o generated_images
```

#### FEM Simulation
```bash
cd FEM_simulation_SOFAscene
python ContactTest_single.py  # Single contact simulation
python ContactTest_in_loop.py # Batch simulation
```

## üìä Data Format

### Dataset Structure
The complete dataset is available at: **[Hugging Face DIGIT Simulation Dataset](https://huggingface.co/datasets/Ndolphin/DIGIT_simulation/tree/main/datasets)**

```
DIGIT_simulation_dataset/
‚îú‚îÄ‚îÄ datasets/
‚îÇ   ‚îú‚îÄ‚îÄ NodalDataOutput.zip          # Nodal displacement data
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ D21119/                  # Sensor ID
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ hammar/              # Object name
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ topROI_step_000.csv
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ topROI_step_001.csv
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ mug/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ D21242/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ D21273/
‚îÇ   ‚îú‚îÄ‚îÄ DeltaImages_100x75.zip       # Tactile visualization images
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ D21119/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ hammar/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ hammar_000.jpg
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ hammar_001.jpg
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ   ‚îî‚îÄ‚îÄ ContactMasks_100x75.zip      # Binary contact masks
‚îÇ       ‚îú‚îÄ‚îÄ D21119/
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ hammar/
‚îÇ       ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ hammar_000_mask.jpg
‚îÇ       ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ hammar_001_mask.jpg
‚îÇ       ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ       ‚îî‚îÄ‚îÄ ...
```

### Input Data
- **CSV Files**: Nodal displacement data with columns [x, y, dz]
- **RGB Images**: Delta images showing tactile sensor changes
- **Contact Masks**: Binary masks indicating contact regions

### Coordinate System
- **X Range**: [-7.5mm, +7.5mm] (sensor width)
- **Y Range**: [0mm, 20mm] (sensor height)
- **Resolution**: 100√ó75 pixels (rendering), 320√ó240 pixels (perception)

### Data Loading Example
```python
import pandas as pd
from PIL import Image
import numpy as np

# Load nodal data
df = pd.read_csv("topROI_step_000.csv")
x_coords = df["x"].values  # Node X coordinates
y_coords = df["y"].values  # Node Y coordinates  
dz_values = df["dz"].values  # Displacement values

# Load corresponding delta image
delta_img = Image.open("hammar_000.jpg")
delta_array = np.array(delta_img)  # Shape: (75, 100, 3)

# Load contact mask
mask_img = Image.open("hammar_000_mask.jpg")
mask_array = np.array(mask_img) > 0  # Binary mask
```

## üîß Model Details

### Hyperparameters

| Parameter | Perception Network | Rendering Network |
|-----------|-------------------|-------------------|
| Input Size | 320√ó240√ó3 | 19√ó75√ó100 |
| Output Size | 320√ó240√ó3 | 3√ó75√ó100 |
| Batch Size | 4 | 8 |
| Learning Rate | 1e-3 | 2e-4 |
| Optimizer | Adam | AdamW |
| Loss Function | MSE (dz only) | Masked L1 |
| Epochs | 50 | 50 |

### Loss Functions

**Perception Network:**
```python
loss = MSE(predicted_dz, target_dz)  # Focus on displacement only
```

**Rendering Network:**
```python
loss = contact_loss + 0.1 * background_loss + 0.02 * neutrality_loss
```

## ‚ö° Performance

### Inference Speed (RTX 5090)
- **Perception Network**: ~0.35ms per image (FP32), ~0.22ms (FP16)
- **Rendering Network**: ~0.20ms per image (FP32), ~0.12ms (FP16)
- **Throughput**: 2k-10k+ images/second depending on batch size

### Model Complexity
- **Perception Network**: ~500K-1M parameters
- **Rendering Network**: ~200K parameters (lightweight design)

## üìà Results

The framework enables:
- **High-fidelity** tactile simulation and reconstruction
- **Real-time** inference capabilities
- **Bidirectional** mapping between physical and visual domains
- **Robust** generalization to unseen objects and contact scenarios

## üî¨ Research Applications

- **Tactile Sensing**: Enhanced understanding of touch-based perception
- **Robotics**: Improved manipulation through tactile feedback
- **Material Science**: Analysis of contact mechanics and deformation
- **Human-Computer Interaction**: Natural touch interfaces

## üìö Citation

If you use this work in your research, please cite:

```bibtex
@article{digit_simulation_2025,
  title={DIGIT Sensor Simulation and Deep Learning Framework},
  author={[Your Name]},
  journal={[Journal Name]},
  year={2025},
  note={Available at: https://github.com/ndolphin-github/DIGIT_simulation}
}
```

## ü§ù Contributing

Contributions are welcome! Please feel free to submit a Pull Request. For major changes, please open an issue first to discuss what you would like to change.

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## üôè Acknowledgments

- SOFA Framework for FEM simulation capabilities
- PyTorch team for the deep learning framework
- Contributors to the tactile sensing research community

## üìû Contact

For questions and support, please open an issue on GitHub or contact [ndolphin93@gmail.com].

---

‚≠ê **Star this repository if you find it useful!**